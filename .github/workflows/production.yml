name: Production

on:
  # Trigger the workflow on push on the master branch, or for any pull request
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

concurrency:
  # Cancel all workflows that are stil running if any when updating branches associated with PRs,
  # BUT don't do anything for workflows that are not triggered by PRs.
  group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

env:
  # Note that secrets are not passed to workflows that are triggered by a pull request from a fork
  HF_TOKEN: ${{ secrets.HF_TOKEN }}
  HF_HUB_DOWNLOAD_TIMEOUT: 60
  GENESIS_IMAGE_VER: "1_14"
  TIMEOUT_MINUTES: 60
  FORCE_COLOR: 1
  PY_COLORS: 1
  MADRONA_DISABLE_CUDA_HEAP_SIZE: "1"
  OMNI_KIT_ACCEPT_EULA: "yes"
  OMNI_KIT_ALLOW_ROOT: "1"

jobs:
  # unit-tests:
  #   name: production-unit_tests-${{ matrix.GS_ENABLE_NDARRAY == '0' && 'field' || 'ndarray' }}

  #   runs-on: [self-hosted, coreweave, genesis-world]

  #   strategy:
  #     fail-fast: true
  #     max-parallel: 1
  #     matrix:
  #       GS_ENABLE_NDARRAY: ["0", "1"]

  #   env:
  #     GS_ENABLE_NDARRAY: ${{ matrix.GS_ENABLE_NDARRAY }}

  #   steps:
  #     - name: Checkout code
  #       uses: actions/checkout@v4
  #     - name: Run unit tests
  #       if: github.event_name == 'pull_request'
  #       run: |
  #         SLURM_JOB_NAME="$(uuidgen)_$(date +%Y%m%d_%H%M%S)"
  #         echo "SLURM_JOB_NAME=${SLURM_JOB_NAME}" >> $GITHUB_ENV

  #         mkdir -p "${HOME}/.cache" "${HOME}/.venv"

  #         # TODO: USD baking does not currently support Python 3.11 since
  #         # NVIDIA does not currently release `omniverse-kit==107.3` on PyPI.
  #         # See: https://github.com/Genesis-Embodied-AI/Genesis/pull/1300
  #         srun \
  #           --container-image="/mnt/data/images/genesis-v${GENESIS_IMAGE_VER}.sqsh" \
  #           --container-mounts=\
  #         "${HOME}/.venv":/root/.venv,\
  #         "${HOME}/.cache":/root/.cache,\
  #         "${{ github.workspace }}":/root/workspace \
  #           --no-container-mount-home --container-workdir=/root/workspace \
  #           --export=NVIDIA_DRIVER_CAPABILITIES=all,BASH_ENV=/root/.bashrc,HF_TOKEN,GS_ENABLE_NDARRAY=${GS_ENABLE_NDARRAY} \
  #           --partition=hpc-mid --nodes=1 --gpus=8 --exclusive --time="${TIMEOUT_MINUTES}" \
  #           --job-name=${SLURM_JOB_NAME} \
  #           bash -e -s << 'EOF'
  #             if test -n "$(find /root/.venv -maxdepth 0 -empty)"; then
  #               python3 -m venv --system-site-packages /root/.venv
  #               source /root/.venv/bin/activate
  #               pip install --no-input --upgrade pip pkg-info wheel
  #               pip install --no-input --ignore-installed --upgrade blinker pyparsing setuptools
  #             fi
  #             source /root/.venv/bin/activate

  #             pip install --no-input --extra-index-url https://pypi.nvidia.com/ omniverse-kit
  #             pip install --no-input ".[dev,render,usd]"

  #             pytest -v -ra --backend gpu --dev --forked ./tests
  #         EOF
  #     - name: Kill srun job systematically
  #       if: always()
  #       run: |
  #         if [ -n "${SLURM_JOB_NAME}" ] ; then
  #           scancel --user=${USER} --name="${SLURM_JOB_NAME}"
  #         fi

  benchmarks:
    name: production-benchmarks-${{ matrix.GS_ENABLE_NDARRAY == '0' && 'field' || 'ndarray' }}

    # needs: unit-tests
    runs-on: [self-hosted, coreweave, genesis-world]

    strategy:
      matrix:
        GS_ENABLE_NDARRAY: ["0", "1"]

    env:
      # Note that secrets are not passed to workflows that are triggered by a pull request from a fork
      WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
      GS_ENABLE_NDARRAY: ${{ matrix.GS_ENABLE_NDARRAY }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          # Checkout full history is required to shallow cloning while mark HEAD as "grafted". This breaks remote
          # tracking thereby making it impossible to detect whether a commit is contained in upstream main.
          fetch-depth: 0
      - name: Allocate SLURM slot and start container session
        run: |
          # Force regeneration of names (important for step retries)
          unset SLURM_JOB_NAME CONTAINER_NAME SLURM_JOB_ID
          
          SLURM_JOB_NAME="$(uuidgen)_$(date +%Y%m%d_%H%M%S)"
          CONTAINER_NAME="genesis-${SLURM_JOB_NAME}"
          echo "SLURM_JOB_NAME=${SLURM_JOB_NAME}" >> $GITHUB_ENV
          echo "CONTAINER_NAME=${CONTAINER_NAME}" >> $GITHUB_ENV

          salloc --job-name="${SLURM_JOB_NAME}" \
                 --partition=hpc-mid --nodes=1 --gpus=8 --exclusive \
                 --time="${TIMEOUT_MINUTES}" \
                 bash -c "sleep ${TIMEOUT_MINUTES}m" &
          echo "SLURM_ALLOC_PID=$!" >> $GITHUB_ENV

          for i in $(seq 1 30); do
            SLURM_JOB_ID="$(squeue --noheader -o '%A' --name "${SLURM_JOB_NAME}" | head -n1)"
            test -n "$SLURM_JOB_ID" && break
            sleep 3
          done
          if [ -z "$SLURM_JOB_ID" ]; then
            echo "Failed to obtain SLURM_JOB_ID" >&2
            exit 1
          fi
          echo "SLURM_JOB_ID=${SLURM_JOB_ID}" >> $GITHUB_ENV
          
          # Get the allocated node name
          SLURM_NODE="$(squeue --noheader -o '%N' -j "${SLURM_JOB_ID}")"
          echo "Allocated node: ${SLURM_NODE}"

          # Base container configuration (mounts and workdir)
          SRUN_CONTAINER_OPTS="--container-mounts=/mnt/data/artifacts:/mnt/data/artifacts,${{ github.workspace }}:/root/workspace,${HOME}/.cache/pip:/root/.cache/pip"
          SRUN_CONTAINER_OPTS="${SRUN_CONTAINER_OPTS} --no-container-mount-home"
          SRUN_CONTAINER_OPTS="${SRUN_CONTAINER_OPTS} --container-workdir=/root/workspace"
          
          # For initial container start: minimal exports to avoid BASH_ENV issues during bootstrap
          # Don't use --container-name for initial creation, let pyxis auto-name it
          SRUN_INIT="${SRUN_CONTAINER_OPTS} --export=NVIDIA_DRIVER_CAPABILITIES=all"
          
          # For subsequent commands: attach to the same container by omitting --container-image
          # All srun commands in the same job will use the same container automatically
          SRUN_COMMON_BARE="--jobid=${SLURM_JOB_ID} ${SRUN_CONTAINER_OPTS} --export=NVIDIA_DRIVER_CAPABILITIES=all,BASH_ENV=/root/.bashrc,HF_TOKEN,WANDB_API_KEY,GS_ENABLE_NDARRAY=${GS_ENABLE_NDARRAY}"
          echo "SRUN_COMMON_BARE=${SRUN_COMMON_BARE}" >> "$GITHUB_ENV"

          SRUN_COMMON="--overlap ${SRUN_COMMON_BARE}"
          echo "SRUN_COMMON=${SRUN_COMMON}" >> "$GITHUB_ENV"

          # Start the container once (with image) and keep it alive
          # Use the existing job allocation but let pyxis create a new container
          srun --jobid=${SLURM_JOB_ID} \
               --container-image="/mnt/data/images/genesis-v${GENESIS_IMAGE_VER}.sqsh" \
               ${SRUN_INIT} \
               sleep "${TIMEOUT_MINUTES}m" &
          echo "BASE_SRUN_PID=$!" >> $GITHUB_ENV
          BASE_SRUN_PID=$!
          
          # Wait for container to be fully initialized
          for i in $(seq 1 30); do
            # Check if the background srun process is still alive
            if ! kill -0 "${BASE_SRUN_PID}" 2>/dev/null; then
              echo "ERROR: Container initialization failed (srun process died)" >&2
              wait "${BASE_SRUN_PID}" || true  # Show exit code
              exit 1
            fi
            
            # Try to run a simple command in the container
            if srun ${SRUN_COMMON} /bin/true 2>/dev/null; then
              echo "Container is ready"
              break
            fi
            echo "Waiting for container to be ready (attempt $i/30)..."
            sleep 2
          done
          
          # Final check: if we exhausted retries, fail
          if ! srun ${SRUN_COMMON} /bin/true 2>/dev/null; then
            echo "ERROR: Container failed to become ready after 30 attempts" >&2
            exit 1
          fi

      - name: Build
        run: |
          srun ${SRUN_COMMON} \
               bash .github/workflows/scripts/production_build.sh

      - name: Test
        run: |
          srun ${SRUN_COMMON} bash -e -s <<'EOF'
            source /dev/shm/venv/bin/activate
            pytest --print -x -m "benchmarks" ./tests
            cat speed_test*.txt > "/mnt/data/artifacts/speed_test_${SLURM_JOB_NAME}.txt"
          EOF

      - name: Cleanup allocation
        if: always()
        run: |
          # Explicitly remove the container before killing the job
          if [ -n "${SLURM_JOB_ID}" ] && [ -n "${CONTAINER_NAME}" ]; then
            srun --jobid=${SLURM_JOB_ID} enroot remove -f "${CONTAINER_NAME}" 2>/dev/null || true
          fi
          kill "${BASE_SRUN_PID}" 2>/dev/null || true
          scancel "${SLURM_JOB_ID}" 2>/dev/null || true
          kill "${SLURM_ALLOC_PID}" 2>/dev/null || true
      - name: Display benchmark stats
        run: |
          cat "/mnt/data/artifacts/speed_test_${SLURM_JOB_NAME}.txt"
      - name: Upload benchmark stats as artifact
        uses: actions/upload-artifact@v4
        with:
          name: speed-test-${{ matrix.GS_ENABLE_NDARRAY }}
          path: "/mnt/data/artifacts/speed_test_${{ env.SLURM_JOB_NAME }}.txt"
