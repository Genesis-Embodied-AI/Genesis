name: Benchmark Comparison & Alarm Regression

on:
  workflow_run:
    workflows: ["Production"]
    types: [completed]

permissions:
  contents: read
  actions: read
  pull-requests: write
  checks: write

jobs:
  comment-if-regressed:
    runs-on: ubuntu-latest
    if: >
      github.event.workflow_run.event == 'pull_request' &&
      contains(fromJson('["success","neutral"]'), github.event.workflow_run.conclusion)

    steps:
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install deps
        run: |
          python -m pip install --quiet --upgrade wandb

      - name: Download artifacts from triggering run
        id: dl
        uses: actions/download-artifact@v4
        with:
          pattern: speed-test-*
          run-id: ${{ github.event.workflow_run.id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          path: ./artifacts

      - name: Show downloaded files
        run: |
          echo "Downloaded into ${{ steps.dl.outputs.download-path }}"
          ls -la ${{ steps.dl.outputs.download-path }} || true
          (command -v tree >/dev/null && tree -a ${{ steps.dl.outputs.download-path }}) || true

      - name: Check regressions + build outputs
        id: analyze
        env:
          # Note that secrets are not passed to workflows that are triggered by a pull request from a fork
          # --- W&B ---
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
          WANDB_ENTITY: genesis-ai-company
          WANDB_PROJECT: genesis-benchmarks
          WANDB_SILENT: "true"

          # --- Parameters ---
          MAX_VALID_REVISIONS: 5
          MAX_FETCH_REVISIONS: 100
          RUNTIME_REGRESSION_TOLERANCE_PCT: 10
          COMPILE_REGRESSION_TOLERANCE_PCT: 10

          # Input/Output paths
          ARTIFACTS_DIR: ${{ steps.dl.outputs.download-path }}
          PR_COMMENT_PATH: pr_comment.md
          CHECK_BODY_PATH: check_output.md
          CSV_RUNTIME_PATH: runtime_fps.csv
          CSV_COMPILE_PATH: compile_time.csv
          TABLES_JSONL_PATH: benchmark_tables.jsonl
          EXIT_CODE_REGRESSION: 42
        run: |
          { python - << 'PY'; EXIT_CODE=$?; } || true

          import os, sys, json, re, math, statistics
          import wandb
          from pathlib import Path
          import csv

          # ----- arguments -----

          MAX_VALID_REVISIONS = int(os.environ["MAX_VALID_REVISIONS"])
          MAX_FETCH_REVISIONS = int(os.environ["MAX_FETCH_REVISIONS"])

          METRICS_TOL = {
            "runtime_fps": float(os.environ["RUNTIME_REGRESSION_TOLERANCE_PCT"]),
            "compile_time": float(os.environ["COMPILE_REGRESSION_TOLERANCE_PCT"]),
          }

          artifacts_dir = Path(os.environ["ARTIFACTS_DIR"]).expanduser().resolve()

          pr_comment_path = Path(os.environ["PR_COMMENT_PATH"]).expanduser()
          check_body_path = Path(os.environ["CHECK_BODY_PATH"]).expanduser()

          csv_runtime = Path(os.environ["CSV_RUNTIME_PATH"]).expanduser().resolve()
          csv_compile = Path(os.environ["CSV_COMPILE_PATH"]).expanduser().resolve()
          jsonl_path  = Path(os.environ["TABLES_JSONL_PATH"]).expanduser().resolve()

          # ---------- helpers ----------

          METRIC_KEYS = ("compile_time", "runtime_fps", "realtime_factor")

          def _normalize_kv_id(kv: dict) -> str:
              return "-".join(f"{k}={v}" for k, v in sorted(kv.items()))

          def normalize_benchmark_id(bid: str) -> str:
              kv = dict(map(str.strip, token.split("=", 1)) for token in bid.split("-"))
              return _normalize_kv_id(kv)

          def artifacts_parse_csv_summary(current_txt_path):
              out = {}
              for line in current_txt_path.read_text().splitlines():
                  kv = dict(map(str.strip, p.split("=", 1)) for p in line.split("|") if "=" in p)
                  record = {}
                  for k in METRIC_KEYS:
                      try:
                          record[k] = float(kv.pop(k))
                      except (ValueError, TypeError, KeyError):
                          pass
                  bid = _normalize_kv_id(kv)
                  out[bid] = record
              return out

          def fmt_num(v, is_int: bool):
              return f"{int(v):,}" if is_int else f"{v:.2f}"

          # ----- load artifacts (current results) -----

          current_csv_paths = list(artifacts_dir.rglob("speed_test*.txt"))
          if not current_csv_paths:
              pr_comment_path.touch()
              check_body_path.touch()
              sys.exit(0)

          current_bm = {}
          for csv_path in current_csv_paths:
              current_bm |= artifacts_parse_csv_summary(csv_path)
          bids_set = set(current_bm.keys())
          assert bids_set

          # ----- W&B baselines -----

          if not "WANDB_API_KEY" in os.environ:
              print("WANDB_API_KEY is not set")
              sys.exit(0)
          ENTITY = os.environ["WANDB_ENTITY"]
          PROJECT = os.environ["WANDB_PROJECT"]

          api = wandb.Api()
          runs_iter = api.runs(f"{ENTITY}/{PROJECT}", order="-created_at")

          is_complete = False
          records_by_rev = {}
          for i, run in enumerate(runs_iter):
              # Abort if still not complete after checking enough runs.
              # This would happen if a new benchmark has been added, and not enough past data is available yet.
              if i == MAX_FETCH_REVISIONS:
                  break

              # Early return if enough complete records have been collected
              records_is_complete = [bids_set.issubset(record.keys()) for record in records_by_rev.values()]
              is_complete = sum(records_is_complete) == MAX_VALID_REVISIONS
              if is_complete:
                  break

              # Skip runs did not finish for some reason
              if run.state != "finished":
                  continue

              # Load config and summary, with support of legacy runs
              config, summary = run.config, run.summary
              if isinstance(config, str):
                  config = {k: v["value"] for k, v in json.loads(run.config).items() if not k.startswith("_")}
              if isinstance(summary._json_dict, str):
                  summary = json.loads(summary._json_dict)

              # Extract revision commit and branch
              try:
                  rev, branch = config["revision"].split("@", 1)
              except ValueError:
                  # Ignore this run if the revision has been corrupted for some unknown reason
                  continue
              # Ignore runs associated with a commit that is not part of the official repository
              if not branch.startswith('Genesis-Embodied-AI/'):
                  continue

              # Do not store new records if the desired number of revision is already reached
              if len(records_by_rev) == MAX_VALID_REVISIONS and rev not in records_by_rev:
                  continue

              # Extract benchmark ID and normalize it to make sure it does not depends on key ordering.
              # Note that the rigid body benchmark suite is the only one being supported for now.
              sid, bid = config["benchmark_id"].split("-", 1)
              nbid = normalize_benchmark_id(bid)
              if sid != "rigid_body":
                  continue

              # Make sure that stats are valid
              try:
                  is_valid = True
                  for k in METRIC_KEYS:
                      v = summary[k]
                      if not isinstance(v, (float, int)) or math.isnan(v):
                          is_valid = False
                          break
                  if not is_valid:
                      continue
              except KeyError:
                  continue

              # Store all the records into a dict
              records_by_rev.setdefault(rev, {})[nbid] = {
                  metric: summary[metric] for metric in METRIC_KEYS
              }

          # ----- build TWO tables -----
          # Parse benchmark IDs into key-value dicts
          def parse_norm_id(nbid: str) -> dict:
              kv = {}
              if nbid:
                  for token in nbid.split("-"):
                      token = token.strip()
                      if token and "=" in token:
                          k, v = token.split("=", 1)
                          kv[k.strip()] = v.strip()
              return kv

          id2kv = {bid: parse_norm_id(bid) for bid in current_bm.keys()}
          all_keys = set()
          for kv in id2kv.values():
              all_keys.update(kv.keys())
          ordered_keys = sorted(all_keys)

          reg_found = False
          tables = {}
          rows_for_csv = {
              "runtime_fps": [],
              "compile_time": []
          }
          for metric, alias in (("runtime_fps", "FPS"), ("compile_time", "compile")):
              rows_md = []

              header_cells = ["status"] + ordered_keys + [f"current {alias}", f"baseline {alias} [last (mean ¬± std)]", f"Œî {alias}"]
              header = "| " + " | ".join(header_cells) + " |"
              align  = "|:------:|" + "|".join([":---" for _ in ordered_keys]) + "|---:|---:|---:|"

              for bid in sorted(current_bm.keys()):
                  value_cur = current_bm[bid][metric]
                  is_int = isinstance(value_cur, int) or value_cur.is_integer()
                  value_repr = fmt_num(value_cur, is_int)

                  values_prev = [
                      record[bid][metric]
                      for record in records_by_rev.values()
                      if bid in record
                  ]
                  is_reg = False
                  if values_prev:
                      value_ref = statistics.fmean(values_prev)
                      delta = (value_cur - value_ref) / value_ref * 100.0

                      stats_repr = f"{fmt_num(values_prev[0], is_int)}"
                      delta_repr = f"{delta:+.1f}%"
                      if len(values_prev) == MAX_VALID_REVISIONS:
                          value_std = statistics.stdev(values_prev)
                          stats_repr += f" ({fmt_num(value_ref, is_int)} ¬± {fmt_num(value_std, is_int)})"
                          if abs(delta) > METRICS_TOL[metric]:
                              delta_repr = f"**{delta_repr}**"
                              picto = "üî¥"
                              reg_found = True
                              is_reg = True
                          else:
                              picto = "‚úÖ"
                      else:
                          picto = "‚ÑπÔ∏è"
                  else:
                      picto, stats_repr, delta_repr = "‚ÑπÔ∏è", "---", "---"
                      value_ref, delta = None, None

                  kv = id2kv[bid]
                  key_cells = [kv.get(k, "-") for k in ordered_keys]

                  rows_md.append("| " + " | ".join([picto] + key_cells + [value_repr, stats_repr, delta_repr]) + " |")

                  # CSV/JSONL
                  rows_for_csv[metric].append({
                      "metric": metric,
                      "benchmark_id": bid,
                      "status": "regression" if is_reg else ("not_enough_baselines" if (len(values_prev) < MAX_VALID_REVISIONS) else "ok"),
                      **{k: kv.get(k, "-") for k in ordered_keys},
                      "current": float(value_cur) if isinstance(value_cur,(int,float)) else None,
                      "baseline_mean": float(value_ref) if isinstance(value_ref,(int,float)) else None,
                      "delta_pct": float(delta) if isinstance(delta,(int,float)) else None,
                  })

              tables[metric] = [header, align] + rows_md

          # ----- baseline commit list (MD) -----
          blist = [f"- Commit {i}: {sha}" for i, sha in enumerate(records_by_rev.keys(), 1)]
          baseline_block = ["**Baselines considered:** " + f"**{len(records_by_rev)}** commits"] + blist

          # ----- CHECK body (always) -----

          thr_repr = ", ".join(
              f"{alias} ¬± {METRICS_TOL[metric]:.0f}%"
              for metric, alias in (("runtime_fps", "runtime"), ("compile_time", "compile"))
          )

          check_body = "\n".join(
              [
                  *baseline_block,
                  "",
                  f"Thresholds: {thr_repr}",
                  "",
                  "### Runtime FPS",
                  *tables["runtime_fps"],
                  "",
                  "### Compile Time",
                  *tables["compile_time"],
              ]
          )

          # ----- COMMENT body (only if regressions) -----
          if reg_found:
              comment_body = "\n".join([":warning: **Benchmark Regression Detected**", *check_body])
          else:
              comment_body = ""

          # common CSV header
          csv_header = (
              ["metric", "benchmark_id", "status"]
              + ordered_keys
              + ["current", "baseline_last", "baseline_mean", "baseline_std", "delta_pct"]
          )

          # runtime_fps.csv
          with csv_runtime.open("w", newline="", encoding="utf-8") as f:
              w = csv.DictWriter(f, fieldnames=csv_header)
              w.writeheader()
              for rec in rows_for_csv["runtime_fps"]:
                  w.writerow(rec)

          # compile_time.csv
          with csv_compile.open("w", newline="", encoding="utf-8") as f:
              w = csv.DictWriter(f, fieldnames=csv_header)
              w.writeheader()
              for rec in rows_for_csv["compile_time"]:
                  w.writerow(rec)

          # JSONL (all two metrics)
          with jsonl_path.open("w", encoding="utf-8") as f:
              for m in ("runtime_fps", "compile_time"):
                  for rec in rows_for_csv[m]:
                      f.write(json.dumps(rec, ensure_ascii=False) + "\n")

          # write md results
          check_body_path.write_text(check_body+"\n", encoding="utf-8")
          pr_comment_path.write_text(comment_body+"\n", encoding="utf-8")

          # Exit with 42 if regressions are found, 0 otherwise
          sys.exit(int(os.environ["EXIT_CODE_REGRESSION"]) if reg_found else 0)
          PY

          # Enable command trace to ease debugging
          set -o xtrace

          # Expose outputs to later steps
          if [ -f "$CHECK_BODY_PATH" ]; then
            {
              echo 'CHECK_OUTPUT<<__EOF__'
              cat "$CHECK_BODY_PATH"
              echo '__EOF__'
            } >> "$GITHUB_ENV"
          else
            echo "CHECK_OUTPUT=" >> "$GITHUB_ENV"
          fi

          # Only set SCRIPT_OUTPUT when we actually want to comment
          if [ "$EXIT_CODE" = "$EXIT_CODE_REGRESSION" ] && [ -s "$PR_COMMENT_PATH" ]; then
            {
              echo 'SCRIPT_OUTPUT<<__EOF__'
              cat "$PR_COMMENT_PATH"
              echo '__EOF__'
            } >> "$GITHUB_ENV"
          else
            echo "SCRIPT_OUTPUT=" >> "$GITHUB_ENV"
          fi

          # Export status
          echo "CONCLUSION=$([ "$EXIT_CODE" = "0" ] && echo 'success' || echo 'failure')" >> "$GITHUB_ENV"
          echo "HAS_REGRESSIONS=$([ "$EXIT_CODE" = "$EXIT_CODE_REGRESSION" ] && echo 1 || echo 0)" >> "$GITHUB_ENV"

      - name: Add PR comment
        if: ${{ env.SCRIPT_OUTPUT != '' }}
        uses: actions/github-script@v8
        env:
          COMMENT_BODY: ${{ env.SCRIPT_OUTPUT }}
        with:
          script: |
            // Getting PR number when using 'workflow_run' is tricky. For reference, see:
            // * https://docs.github.com/en/webhooks/webhook-events-and-payloads#workflow_run
            // * https://stackoverflow.com/a/75420270/4820605
            const { data } = await github.rest.repos.listPullRequestsAssociatedWithCommit({
              owner: context.payload.workflow_run.head_repository.owner.login,
              repo: context.payload.workflow_run.head_repository.name,
              commit_sha: context.payload.workflow_run.head_sha,
            });
            if (!data || !data.length) {
              core.info('No associated PR; skipping comment.');
              return;
            }
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: data[0].number,
              body: process.env.COMMENT_BODY
            });

      - name: Publish PR check
        if: always()
        uses: actions/github-script@v8
        env:
          CHECK_NAME: Benchmark Comparison
          CHECK_BODY: ${{ env.CHECK_OUTPUT }}
          CONCLUSION: ${{ env.CONCLUSION }}
          HAS_REGRESSIONS: ${{ env.HAS_REGRESSIONS }}
        with:
          script: |
            const summary = (process.env.HAS_REGRESSIONS || '0') === '1'
              ? 'üî¥ Regressions detected. See tables below.'
              : '‚úÖ No regressions detected. See tables below.';
            await github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              head_sha: context.payload.workflow_run.head_sha,
              name: process.env.CHECK_NAME,
              status: 'completed',
              conclusion: process.env.CONCLUSION,
              output: {
                title: process.env.CHECK_NAME,
                summary,
                text: process.env.CHECK_BODY || undefined
              }
            });

      - name: Upload benchmark comparisons in CSV & JSONL
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-comparison-tables
          path: |
            runtime_fps.csv
            compile_time.csv
            benchmark_tables.jsonl
          if-no-files-found: warn
