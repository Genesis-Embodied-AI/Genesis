name: Benchmark Comparison & Alarm Regression

on:
  workflow_run:
    workflows: ["Production"]
    types: [completed]

permissions:
  contents: read
  actions: read
  pull-requests: write
  checks: write

jobs:
  comment-if-regressed:
    runs-on: ubuntu-latest
    if: >
      github.event.workflow_run.event == 'pull_request' &&
      contains(fromJson('["success","neutral"]'), github.event.workflow_run.conclusion)

    steps:
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install deps
        run: |
          python -m pip install --quiet --upgrade wandb frozendict

      - name: Download speed artifacts from triggering run
        id: dl_speed
        uses: actions/download-artifact@v4
        with:
          pattern: speed-test-*
          run-id: ${{ github.event.workflow_run.id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          path: ./artifacts

      - name: Download mem artifacts from triggering run
        id: dl_mem
        uses: actions/download-artifact@v4
        with:
          pattern: mem-test-*
          run-id: ${{ github.event.workflow_run.id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          path: ./artifacts

      - name: Show downloaded speed files
        run: |
          echo "Downloaded into ${{ steps.dl_speed.outputs.download-path }}"
          ls -la ${{ steps.dl_speed.outputs.download-path }} || true
          (command -v tree >/dev/null && tree -a ${{ steps.dl_speed.outputs.download-path }}) || true

      - name: Show downloaded mem files
        run: |
          echo "Downloaded into ${{ steps.dl_mem.outputs.download-path }}"
          ls -la ${{ steps.dl_mem.outputs.download-path }} || true
          (command -v tree >/dev/null && tree -a ${{ steps.dl_mem.outputs.download-path }}) || true
  
      - name: Check regressions + build outputs
        id: analyze
        env:
          # Note that secrets are not passed to workflows that are triggered by a pull request from a fork
          # --- W&B ---
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
          WANDB_ENTITY: genesis-ai-company
          WANDB_PROJECT: genesis-benchmarks
          WANDB_SILENT: "true"

          # --- Parameters ---
          MAX_VALID_REVISIONS: 5
          MAX_FETCH_REVISIONS: 40
          RUNTIME_REGRESSION_TOLERANCE_PCT: 8
          COMPILE_REGRESSION_TOLERANCE_PCT: 16
          MEMORY_REGRESSION_TOLERANCE_PCT: 10

          # Input/Output paths
          SPEED_ARTIFACTS_DIR: ${{ steps.dl_speed.outputs.download-path }}
          MEM_ARTIFACTS_DIR: ${{ steps.dl_mem.outputs.download-path }}
          CHECK_BODY_PATH: check_output.md
          CSV_RUNTIME_PATH: runtime_fps.csv
          CSV_COMPILE_PATH: compile_time.csv
          CSV_MEM_PATH: mem.csv
          EXIT_CODE_REGRESSION: 42
          EXIT_CODE_ALERT: 43
        run: |
          { python - << 'PY'; EXIT_CODE=$?; } || true

          import os, sys, json, re, math, statistics
          import wandb
          from frozendict import frozendict
          from pathlib import Path
          import csv

          # ----- arguments -----

          MAX_VALID_REVISIONS = int(os.environ["MAX_VALID_REVISIONS"])
          MAX_FETCH_REVISIONS = int(os.environ["MAX_FETCH_REVISIONS"])

          METRICS_TOL = {
            "runtime_fps": float(os.environ["RUNTIME_REGRESSION_TOLERANCE_PCT"]),
            "compile_time": float(os.environ["COMPILE_REGRESSION_TOLERANCE_PCT"]),
            "max_mem_mb": float(os.environ["MEMORY_REGRESSION_TOLERANCE_PCT"]),
          }

          speed_artifacts_dir = Path(os.environ["SPEED_ARTIFACTS_DIR"]).expanduser().resolve()
          mem_artifacts_dir = Path(os.environ["MEM_ARTIFACTS_DIR"]).expanduser().resolve()
          check_body_path = Path(os.environ["CHECK_BODY_PATH"]).expanduser()

          csv_files = {
            "runtime_fps": Path(os.environ["CSV_RUNTIME_PATH"]).expanduser().resolve(),
            "compile_time": Path(os.environ["CSV_COMPILE_PATH"]).expanduser().resolve(),
            "mem": Path(os.environ["CSV_MEM_PATH"]).expanduser().resolve(),
          }

          # ---------- helpers ----------

          METRIC_KEYS = ("compile_time", "runtime_fps", "realtime_factor")

          def parse_benchmark_id(bid: str) -> dict:
              kv = {}
              if bid:
                  for token in bid.split("-"):
                      token = token.strip()
                      if token and "=" in token:
                          k, v = token.split("=", 1)
                          kv[k.strip()] = v.strip()
              return kv

          def normalize_benchmark_id(bid: str) -> frozendict[str, str]:
              return frozendict(parse_benchmark_id(bid))

          def get_param_names(bids: tuple[frozendict]) -> tuple[str, ...]:
              """
              Merge a list of tuples into a single tuple of keys that:
              - Preserves the relative order of keys within each tuple
              - Gives precedence to later tuples when conflicts arise
              """
              merged = list(bids[-1])
              merged_set = set(merged)
              for tup in bids[:-1]:
                  for key in tup:
                      if key not in merged_set:
                          merged.append(key)
                          merged_set.add(key)
              return tuple(merged)

          def artifacts_parse_csv_summary(current_txt_path):
              out = {}
              for line in current_txt_path.read_text().splitlines():
                  kv = dict(map(str.strip, p.split("=", 1)) for p in line.split("|") if "=" in p)
                  record = {}
                  for k in METRIC_KEYS:
                      try:
                          record[k] = float(kv.pop(k))
                      except (ValueError, TypeError, KeyError):
                          pass
                  nbid = frozendict(kv)
                  out[nbid] = record
              return out

          def parse_mem_csv(mem_csv_path):
              """Parse memory CSV file format: test,max_mem_mb"""
              out = {}
              with mem_csv_path.open() as f:
                  reader = csv.DictReader(f)
                  for row in reader:
                      # Extract test name and parse it
                      # Format: test_speed[scenario-solver-flag-n_envs-backend]
                      test_name = row["test"]
                      # Extract the part between brackets
                      if "[" in test_name and "]" in test_name:
                          params_str = test_name[test_name.index("[")+1:test_name.index("]")]
                          # Split by '-' to get individual params
                          parts = params_str.split("-")
                          if len(parts) >= 5:
                              scenario, solver, flag, n_envs, backend = parts[0], parts[1], parts[2], parts[3], parts[4]
                              # Create normalized dict
                              kv = {
                                  "scenario": scenario,
                                  "solver": solver if solver != "None" else None,
                                  "flag": flag if flag != "None" else None,
                                  "n_envs": n_envs,
                                  "backend": backend,
                              }
                              # Remove None values for consistency
                              kv = {k: v for k, v in kv.items() if v is not None}
                              nbid = frozendict(kv)
                              try:
                                  out[nbid] = {"max_mem_mb": float(row["max_mem_mb"])}
                              except (ValueError, TypeError, KeyError):
                                  pass
              return out

          def fmt_num(v, is_int: bool):
              return f"{int(v):,}" if is_int else f"{v:.2f}"

          def fetch_wandb_baselines(api, entity, project, bids_set, suite_name, metric_keys, max_valid_revisions, max_fetch_revisions):
              """
              Fetch baseline data from W&B for a given benchmark suite.
              
              Args:
                  api: W&B API instance
                  entity: W&B entity name
                  project: W&B project name
                  bids_set: Set of benchmark IDs to look for
                  suite_name: Benchmark suite name (e.g., "rigid_body", "memory")
                  metric_keys: Tuple of metric keys to extract (e.g., ("compile_time", "runtime_fps") or ("max_mem_mb",))
                  max_valid_revisions: Maximum number of complete revisions to collect
                  max_fetch_revisions: Maximum number of runs to fetch before giving up
              
              Returns:
                  dict: Records by revision {rev: {bid: {metric: value, ...}, ...}, ...}
              """
              runs_iter = api.runs(f"{entity}/{project}", order="-created_at")
              
              revs = set()
              records_by_rev = {}
              
              for i, run in enumerate(runs_iter):
                  # Abort if still not complete after checking enough runs
                  if len(revs) == max_fetch_revisions:
                      break
                  
                  # Early return if enough complete records have been collected
                  records_is_complete = [bids_set.issubset(record.keys()) for record in records_by_rev.values()]
                  if sum(records_is_complete) == max_valid_revisions:
                      break
                  
                  # Load config and summary, with support of legacy runs
                  config, summary = run.config, run.summary
                  if isinstance(config, str):
                      config = {k: v["value"] for k, v in json.loads(run.config).items() if not k.startswith("_")}
                  if isinstance(summary._json_dict, str):
                      summary = json.loads(summary._json_dict)
                  
                  # Extract revision commit and branch
                  try:
                      rev, branch = config["revision"].split("@", 1)
                      revs.add(rev)
                  except (ValueError, KeyError):
                      continue
                  
                  # Ignore runs associated with a commit that is not part of the official repository
                  if not branch.startswith('Genesis-Embodied-AI/'):
                      continue
                  
                  # Skip runs that did not finish for some reason
                  if run.state != "finished":
                      continue
                  
                  # Do not store new records if the desired number of revisions is already reached
                  if len(records_by_rev) == max_valid_revisions and rev not in records_by_rev:
                      continue
                  
                  # Extract benchmark ID and normalize it
                  try:
                      sid, bid = config["benchmark_id"].split("-", 1)
                      if sid != suite_name:
                          continue
                  except (ValueError, KeyError):
                      continue
                  
                  # Make sure that stats are valid
                  try:
                      is_valid = True
                      for k in metric_keys:
                          v = summary.get(k)
                          if not isinstance(v, (float, int)) or math.isnan(v):
                              is_valid = False
                              break
                      if not is_valid:
                          continue
                  except (KeyError, TypeError):
                      continue
                  
                  # Store all the records into a dict
                  nbid = normalize_benchmark_id(bid)
                  records_by_rev.setdefault(rev, {})[nbid] = {
                      metric: summary[metric] for metric in metric_keys
                  }
              
              return records_by_rev

          def build_comparison_table(current_data, records_by_rev, params_name, metric_key, metric_alias, sign, tolerance, max_valid_revisions):
              """
              Build a comparison table for a given metric.
              
              Args:
                  current_data: Dict of current benchmark results {bid: {metric: value, ...}, ...}
                  records_by_rev: Historical records by revision
                  params_name: Tuple of parameter names for table columns
                  metric_key: The metric key to analyze (e.g., "runtime_fps", "max_mem_mb")
                  metric_alias: Display name for the metric (e.g., "FPS", "Memory (MB)")
                  sign: 1 if higher is better, -1 if lower is better
                  tolerance: Regression tolerance percentage
                  max_valid_revisions: Number of revisions needed for stats
              
              Returns:
                  tuple: (table_rows, csv_rows, reg_found, alert_found)
              """
              rows_md = []
              rows_csv = []
              reg_found = False
              alert_found = False

              header_cells = (
                  "status",
                  *params_name,
                  f"current {metric_alias}",
                  f"baseline {metric_alias} [last (mean ¬± std)] (*1)",
                  f"Œî {metric_alias} (*2)"
              )
              header = "| " + " | ".join(header_cells) + " |"
              align  = "|:------:|" + "|".join([":---" for _ in params_name]) + "|---:|---:|---:|"

              def local_sort_key(d):
                  key_list = []
                  for col in params_name:
                      if col in d:
                          val = d[col]
                          key_list.append((0, val))
                      else:
                          key_list.append((1, None))
                  return key_list

              for bid in sorted(current_data.keys(), key=local_sort_key):
                  value_cur = current_data[bid][metric_key]
                  is_int = isinstance(value_cur, int) or value_cur.is_integer()
                  value_repr = fmt_num(value_cur, is_int)

                  params_repr = [bid.get(k, "-") for k in params_name]
                  info = {
                      **dict(zip(params_name, params_repr)),
                      "current": value_cur,
                      "baseline_last": None,
                      "baseline_min": None,
                      "baseline_max": None,
                  }

                  values_prev = [
                      record[bid][metric_key]
                      for record in records_by_rev.values()
                      if bid in record
                  ]
                  
                  if values_prev:
                      value_last = values_prev[0]
                      value_ref = statistics.fmean(values_prev)
                      delta = (value_cur - value_last) / value_last * 100.0

                      info["baseline_last"] = int(value_last) if is_int else float(value_last)

                      stats_repr = f"{fmt_num(value_last, is_int)}"
                      delta_repr = f"{delta:+.1f}%"
                      
                      if len(values_prev) == max_valid_revisions:
                          info["baseline_mean"] = int(value_ref) if is_int else float(value_ref)
                          info["baseline_min"] = int(min(values_prev)) if is_int else float(min(values_prev))
                          info["baseline_max"] = int(max(values_prev)) if is_int else float(max(values_prev))

                          value_std = statistics.stdev(values_prev)
                          stats_repr += f" ({fmt_num(value_ref, is_int)} ¬± {fmt_num(value_std, is_int)})"
                          
                          if sign * delta < -tolerance:
                              info["status"] = "regression"
                              delta_repr = f"**{delta_repr}**"
                              picto = "üî¥"
                              reg_found = True
                          elif sign * delta > tolerance:
                              info["status"] = "alert"
                              delta_repr = f"**{delta_repr}**"
                              picto = "‚ö†Ô∏è"
                              alert_found = True
                          else:
                              info["status"] = "ok"
                              picto = "‚úÖ"
                      else:
                          info["status"] = "n/a"
                          picto = "‚ÑπÔ∏è"
                  else:
                      info["status"] = "n/a"
                      picto = "‚ÑπÔ∏è"
                      stats_repr = "---"
                      delta_repr = "---"

                  rows_md.append("| " + " | ".join((picto, *params_repr, value_repr, stats_repr, delta_repr)) + " |")
                  rows_csv.append(info)

              return [header, align] + rows_md, rows_csv, reg_found, alert_found

          # ----- load artifacts (current results) -----

          current_csv_paths = list(speed_artifacts_dir.rglob("speed_test*.txt"))
          if not current_csv_paths:
              check_body_path.touch()
              sys.exit(0)

          current_bm = {}
          for csv_path in current_csv_paths:
              current_bm |= artifacts_parse_csv_summary(csv_path)
          bids_set = frozenset(current_bm.keys())
          assert bids_set

          # ----- W&B baselines -----

          if not "WANDB_API_KEY" in os.environ:
              print("WANDB_API_KEY is not set")
              sys.exit(0)
          ENTITY = os.environ["WANDB_ENTITY"]
          PROJECT = os.environ["WANDB_PROJECT"]

          api = wandb.Api()
          records_by_rev = fetch_wandb_baselines(
              api, ENTITY, PROJECT, bids_set, "rigid_body", METRIC_KEYS,
              MAX_VALID_REVISIONS, MAX_FETCH_REVISIONS
          )

          # ----- load memory artifacts -----

          current_mem_paths = list(mem_artifacts_dir.rglob("mem*.csv"))
          current_mem = {}
          if current_mem_paths:
              for mem_path in current_mem_paths:
                  current_mem |= parse_mem_csv(mem_path)
          mem_bids_set = frozenset(current_mem.keys())

          # ----- W&B memory baselines -----
          
          mem_records_by_rev = {}
          if current_mem:
              # Fetch memory baselines from W&B
              mem_records_by_rev = fetch_wandb_baselines(
                  api, ENTITY, PROJECT, mem_bids_set, "memory", ("max_mem_mb",),
                  MAX_VALID_REVISIONS, MAX_FETCH_REVISIONS
              )

          # ----- build THREE tables -----

          # Parse benchmark IDs into key-value dicts while preserving order
          params_name = get_param_names(tuple((tuple(kv.keys())) for kv in current_bm.keys()))

          reg_found, alert_found = False, False
          tables = {}
          rows_for_csv = {"runtime_fps": [], "compile_time": [], "mem": []}
          
          for metric, alias, sign in (("runtime_fps", "FPS", 1), ("compile_time", "compile", -1)):
              table_rows, csv_rows, metric_reg, metric_alert = build_comparison_table(
                  current_bm, records_by_rev, params_name, metric, alias, sign,
                  METRICS_TOL[metric], MAX_VALID_REVISIONS
              )
              tables[metric] = table_rows
              rows_for_csv[metric] = csv_rows
              reg_found = reg_found or metric_reg
              alert_found = alert_found or metric_alert

          # ----- build memory table -----

          if current_mem:
              mem_params_name = get_param_names(tuple((tuple(kv.keys())) for kv in current_mem.keys()))
              
              table_rows, csv_rows, mem_reg, mem_alert = build_comparison_table(
                  current_mem, mem_records_by_rev, mem_params_name, "max_mem_mb", "Memory (MB)", -1,
                  METRICS_TOL["max_mem_mb"], MAX_VALID_REVISIONS
              )
              tables["mem"] = table_rows
              rows_for_csv["mem"] = csv_rows
              reg_found = reg_found or mem_reg
              alert_found = alert_found or mem_alert

          # ----- baseline commit list (MD) -----
          blist = [f"- Commit {i}: {sha}" for i, sha in enumerate(records_by_rev.keys(), 1)]
          baseline_block = ["**Baselines considered:** " + f"**{len(records_by_rev)}** commits"] + blist

          # ----- CHECK body (always) -----

          thr_repr = ", ".join(
              f"{alias} ¬± {METRICS_TOL[metric]:.0f}%"
              for metric, alias in (("runtime_fps", "runtime"), ("compile_time", "compile"), ("max_mem_mb", "memory"))
          )

          check_body_parts = [
              *baseline_block,
              "",
              f"Thresholds: {thr_repr}",
              "",
              "### Runtime FPS",
              *tables["runtime_fps"],
              "",
              "### Compile Time",
              *tables["compile_time"],
          ]
          
          if current_mem and "mem" in tables:
              check_body_parts.extend([
                  "",
                  "### Memory Usage",
                  *tables["mem"],
              ])
          
          check_body_parts.extend([
              "",
              f"- (*1) last: last commit on main, mean/std: stats over revs {MAX_VALID_REVISIONS} commits if available.",
              f"- (*2) Œî: relative difference between PR and last commit on main, i.e. (PR - main) / main * 100%.",
          ])

          check_body = "\n".join(check_body_parts)

          # ----- COMMENT body (only if regressions) -----

          if reg_found:
              comment_body = "\n".join([":warning: **Benchmark Regression Detected**", *check_body])
          else:
              comment_body = ""

          # CSV file
          for metric in ("runtime_fps", "compile_time", "mem"):
            if not rows_for_csv[metric]:
                continue
            # Collect all possible fieldnames from all records to handle mixed scenarios
            all_fieldnames = set()
            for rec in rows_for_csv[metric]:
                all_fieldnames.update(rec.keys())
            # Sort fieldnames for consistent ordering, with key fields first
            if metric == "mem" and current_mem:
                key_fields = list(mem_params_name)
            else:
                key_fields = list(params_name)
            other_fields = sorted([f for f in all_fieldnames if f not in key_fields])
            fieldnames = key_fields + other_fields
            
            with csv_files[metric].open("w", newline="", encoding="utf-8") as f:
                w = csv.DictWriter(f, fieldnames=fieldnames)
                w.writeheader()
                for rec in rows_for_csv[metric]:
                    w.writerow(rec)

          # write md results
          check_body_path.write_text(check_body + "\n", encoding="utf-8")

          # Exit with error code
          if reg_found:
              exit_code = int(os.environ["EXIT_CODE_REGRESSION"])
          elif alert_found:
              exit_code = int(os.environ["EXIT_CODE_ALERT"])
          else:
              exit_code = 0
          sys.exit(exit_code)
          PY

          # Enable command trace to ease debugging
          set -o xtrace

          # Expose outputs to later steps
          if [ -f "$CHECK_BODY_PATH" ]; then
            {
              echo 'CHECK_OUTPUT<<__EOF__'
              cat "$CHECK_BODY_PATH"
              echo '__EOF__'
            } >> "$GITHUB_ENV"
          else
            echo "CHECK_OUTPUT=" >> "$GITHUB_ENV"
          fi

          # Export status
          echo "HAS_REGRESSIONS=$([ "$EXIT_CODE" = "$EXIT_CODE_REGRESSION" ] && echo 1 || echo 0)" >> "$GITHUB_ENV"
          echo "HAS_ALERTS=$([ "$EXIT_CODE" = "$EXIT_CODE_ALERT" ] && echo 1 || echo 0)" >> "$GITHUB_ENV"

      - name: Upload benchmark comparisons in CSV
        id: upload
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-comparison-tables
          path: |
            runtime_fps.csv
            compile_time.csv
          if-no-files-found: warn

      - name: Publish PR check
        id: publish_check
        uses: actions/github-script@v8
        env:
          CHECK_NAME: Benchmark Comparison
          CHECK_OUTPUT: ${{ env.CHECK_OUTPUT }}
          HAS_REGRESSIONS: ${{ env.HAS_REGRESSIONS }}
          HAS_ALERTS: ${{ env.HAS_ALERTS }}
          ARTIFACT_URL: ${{ steps.upload.outputs.artifact-url }}
        with:
          script: |
            const artifactUrl = process.env.ARTIFACT_URL || '';
            let body = process.env.CHECK_OUTPUT || '';
            if (body && artifactUrl) {
              body += `\n\n**Artifact:** [Download raw data](${artifactUrl})`;
            }

            let summary;
            let conclusion = 'success';
            if ((process.env.HAS_REGRESSIONS || '0') === '1') {
              summary = 'üî¥ Regressions detected. See tables below.';
              conclusion = 'failure';
            } else if ((process.env.HAS_ALERTS || '0') === '1') {
              summary = '‚ö†Ô∏è Large deviation detected. See tables below.';
            } else {
              summary = '‚úÖ No regressions detected. See tables below.';
            }

            const check = await github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              head_sha: context.payload.workflow_run.head_sha,
              name: process.env.CHECK_NAME,
              status: 'completed',
              conclusion: conclusion,
              output: {
                title: process.env.CHECK_NAME,
                summary,
                text: body || undefined
              }
            });
            core.setOutput("check-url", check.data.html_url);

      - name: Add PR comment
        if: ${{ env.HAS_REGRESSIONS == '1' || env.HAS_ALERTS == '1' }}
        uses: actions/github-script@v8
        env:
          HAS_REGRESSIONS: ${{ env.HAS_REGRESSIONS }}
          REPORT_URL: ${{ steps.publish_check.outputs.check-url }}
        with:
          script: |
            // Getting PR number when using 'workflow_run' is tricky. For reference, see:
            // * https://docs.github.com/en/webhooks/webhook-events-and-payloads#workflow_run
            // * https://stackoverflow.com/a/75420270/4820605
            const { data } = await github.rest.repos.listPullRequestsAssociatedWithCommit({
              owner: context.payload.workflow_run.head_repository.owner.login,
              repo: context.payload.workflow_run.head_repository.name,
              commit_sha: context.payload.workflow_run.head_sha,
            });
            if (!data || !data.length) {
              core.info('No associated PR; skipping comment.');
              return;
            }

            const title = (process.env.HAS_REGRESSIONS || '0') === '1'
              ? 'üî¥ Benchmark Regression Detected' : '‚ö†Ô∏è Abnormal Benchmark Result Detected';
            const comment = `**${title} ‚û°Ô∏è [Report](${process.env.REPORT_URL})**`;

            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: data[0].number,
              body: comment
            });
