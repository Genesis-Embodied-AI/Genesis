#!/usr/bin/env python3
"""
Upload memory benchmark results to Weights & Biases.

This script parses the memory CSV file generated by monitor_test_mem.py
and uploads each test's memory usage as a separate W&B run.
"""

import wandb
import csv
import subprocess
import os
import re
import sys
from pathlib import Path


def get_revision():
    """
    Get current git revision in format: commit_hash@org/repo

    Returns:
        str: Revision string like "abc123def456@Genesis-Embodied-AI/Genesis"
    """
    try:
        rev = subprocess.check_output(["git", "rev-parse", "HEAD"], stderr=subprocess.DEVNULL).decode().strip()

        # Get remote URL and parse org/repo
        remote = (
            subprocess.check_output(["git", "remote", "get-url", "origin"], stderr=subprocess.DEVNULL).decode().strip()
        )

        # Parse GitHub URL to get org/repo
        if "github.com" in remote:
            # Handle both SSH (git@github.com:org/repo.git) and HTTPS (https://github.com/org/repo.git)
            org_repo = remote.split("github.com")[-1].strip("/:").replace(".git", "")
            return f"{rev}@{org_repo}"

        return f"{rev}@unknown"
    except subprocess.CalledProcessError:
        return "unknown@unknown"


def parse_test_name(test_name):
    """
    Parse pytest test name to extract benchmark parameters.

    Expected format: test_speed[scenario-solver-flag-n_envs-backend]
    Example: test_speed[franka-None-True-30000-gpu]

    Args:
        test_name: Full test name from pytest

    Returns:
        dict: Parsed parameters, e.g., {scenario: franka, n_envs: 30000, backend: gpu, flag: True}
    """
    # Extract parameters between brackets
    match = re.search(r"\[(.*?)\]", test_name)
    if not match:
        return {}

    parts = match.group(1).split("-")
    if len(parts) < 5:
        return {}

    # Parse the 5 expected parameters
    params = {
        "scenario": parts[0],
        "solver": parts[1],
        "flag": parts[2],
        "n_envs": parts[3],
        "backend": parts[4],
    }

    # Remove "None" values for consistency with alarm.yml parsing
    # This matches the logic in alarm.yml's parse_mem_csv function
    filtered_params = {}
    for k, v in params.items():
        if v != "None" and v is not None:
            filtered_params[k] = v

    return filtered_params


def upload_memory_to_wandb(mem_csv_path):
    """
    Parse memory CSV and upload each test result to W&B.

    Args:
        mem_csv_path: Path to the memory CSV file generated by monitor_test_mem.py
    """

    # Check if W&B is enabled
    if not os.environ.get("WANDB_API_KEY"):
        print("WANDB_API_KEY not set, skipping W&B upload")
        return 0

    if not os.path.exists(mem_csv_path):
        print(f"Memory CSV file not found: {mem_csv_path}")
        return 1

    revision = get_revision()
    print(f"Uploading memory metrics to W&B for revision: {revision}")

    uploaded_count = 0
    skipped_count = 0

    with open(mem_csv_path) as f:
        reader = csv.DictReader(f)

        for row in reader:
            test_name = row["test"]
            max_mem_mb = float(row["max_mem_mb"])

            # Parse test parameters
            params = parse_test_name(test_name)
            if not params:
                print(f"âš ï¸  Skipping {test_name} - couldn't parse parameters")
                skipped_count += 1
                continue

            # Create benchmark ID matching alarm.yml format
            # Format: memory-scenario=franka-n_envs=30000-backend=gpu-flag=True
            benchmark_id = f"memory-{'-'.join(f'{k}={v}' for k, v in params.items())}"

            print(f"ðŸ“Š Uploading {benchmark_id}: {max_mem_mb:.0f} MB")

            try:
                # Create W&B run
                run = wandb.init(
                    project="genesis-benchmarks",
                    name=f"{benchmark_id}-{revision[:12]}",
                    config={
                        "revision": revision,
                        "benchmark_id": benchmark_id,
                        **params,
                    },
                    settings=wandb.Settings(
                        x_disable_stats=True,
                        console="off",
                    ),
                    reinit=True,  # Allow multiple init() calls in same process
                )

                # Log memory metric to summary
                run.log({"max_mem_mb": max_mem_mb})
                run.finish()

                uploaded_count += 1

            except Exception as e:
                print(f"âŒ Failed to upload {benchmark_id}: {e}")
                skipped_count += 1

    print(f"\nâœ… Memory upload complete: {uploaded_count} uploaded, {skipped_count} skipped")
    return 0


def main():
    """Main entry point for the script."""
    if len(sys.argv) < 2:
        print("Usage: python upload_mem_to_wandb.py <mem_csv_path>")
        print("\nExample:")
        print("  python upload_mem_to_wandb.py /path/to/mem_test.csv")
        return 1

    mem_csv_path = sys.argv[1]
    return upload_memory_to_wandb(mem_csv_path)


if __name__ == "__main__":
    sys.exit(main())
