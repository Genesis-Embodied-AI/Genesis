name: Benchmark Comparison & Alarm Regression

# on:
#   workflow_run:
#     workflows: ["Production"]
#     types: [completed]
on:
  pull_request:
    branches:
      - main

permissions:
  contents: read
  actions: read
  pull-requests: write
  checks: write

jobs:
  comment-if-regressed:
    runs-on: ubuntu-latest
    # if: >
    #   github.event.workflow_run.event == 'pull_request' &&
    #   contains(fromJson('["success","neutral"]'), github.event.workflow_run.conclusion)

    env:
      WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}

    steps:
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install deps
        run: |
          python -m pip install --quiet --upgrade wandb

      - name: Setup tmate session
        uses: mxschmitt/action-tmate@v3

      - name: Download artifacts from triggering run
        id: dl
        uses: actions/download-artifact@v4
        with:
          name: speed-test-results
          run-id: ${{ github.event.workflow_run.id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          path: ./artifacts

      - name: Show downloaded files
        run: |
          echo "Downloaded into ${{ steps.dl.outputs.download-path }}"
          ls -la ${{ steps.dl.outputs.download-path }} || true
          (command -v tree >/dev/null && tree -a ${{ steps.dl.outputs.download-path }}) || true

      - name: Resolve PR number
        id: pr
        uses: actions/github-script@v7
        with:
          script: |
            const wr = context.payload.workflow_run;
            let pr = (wr.pull_requests && wr.pull_requests[0]) || null;

            if (!pr) {
              const sha = wr.head_sha;
              const { data } = await github.rest.repos.listPullRequestsAssociatedWithCommit({
                owner: context.repo.owner,
                repo: context.repo.repo,
                commit_sha: sha,
              });
              if (data && data.length) pr = data[0];
            }

            core.setOutput('pr_number', pr ? String(pr.number) : '');

      - name: Check regressions + build outputs
        id: analyze
        if: ${{ steps.pr.outputs.pr_number != '' }}
        env:
          # --- W&B ---
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
          WANDB_ENTITY: genesis-ai-company
          WANDB_PROJECT: genesis-benchmarks
          WANDB_SILENT: "true"

          # --- Parameters ---
          MAX_REVISIONS: "5"
          NO_CHANGE_PATIENCE: "100"
          RUNTIME_REGRESSION_TOLERANCE_PCT: "10"
          COMPILE_REGRESSION_TOLERANCE_PCT: "10"

          # Input/Output paths
          ARTIFACTS_DIR: ${{ steps.dl.outputs.download-path }}
          PR_COMMENT_PATH: pr_comment.md
          CHECK_BODY_PATH: check_output.md
        run: |
          {
          python - <<'PY'
          import os, sys, json, math, re
          import wandb
          from pathlib import Path

          # ---------- helpers ----------
          SHA_RE = re.compile(r"[0-9a-fA-F]{7,40}")
          def _norm_rev(text):
              if not text: return None
              text = text.split("@", 1)[0]
              m = SHA_RE.search(text)
              return m.group(0) if m else text

          def _normalize_kv_id(kv: dict, drop_keys=None) -> str:
              drop = set(drop_keys or [])
              pairs = []
              for k, v in kv.items():
                  if k in drop or v is None:
                      continue
                  k = str(k).strip()
                  v = str(v).strip()
                  if not k or not v:
                      continue
                  pairs.append((k, v))
              pairs.sort(key=lambda x: x[0])
              return "-".join(f"{k}={v}" for k, v in pairs)

          def wandb_normalize_benchmark_id(bid: str) -> str:
              kv = {}
              for token in (p.strip() for p in bid.split("-") if p.strip()):
                  if "=" not in token: continue
                  k, v = token.split("=", 1)
                  kv[k.strip()] = v.strip()
              return _normalize_kv_id(kv)

          def artifacts_parse_speed_txt_lines(lines):
              METRIC_KEYS = {"compile_time", "runtime_fps", "realtime_factor"}
              out = {}
              for line in lines:
                  kv = dict(map(str.strip, p.split("=", 1)) for p in line.split("|") if "=" in p)
                  tid = _normalize_kv_id(kv, drop_keys=METRIC_KEYS)
                  for key in ("runtime_fps", "compile_time"):
                      try:
                          out[tid][key] = float(kv[key])
                      except (ValueError, TypeError, KeyError):
                          pass
              return out

          def fmt_num(v):
              if v is None or (isinstance(v,float) and math.isnan(v)): return "â€”"
              try:
                  if isinstance(v, float) and not v.is_integer():
                      return f"{v:.2f}"
                  return f"{int(v):,}"
              except Exception:
                  return str(v)

          def fmt_pct(v, highlight=False):
              if v is None or (isinstance(v,float) and math.isnan(v)): return "â€”"
              s = f"{v:+.2f}%"
              return f"**{s}**" if highlight else s

          # ----- thresholds -----
          tol_rt = float(os.environ["RUNTIME_REGRESSION_TOLERANCE_PCT"])
          tol_ct = float(os.environ["COMPILE_REGRESSION_TOLERANCE_PCT"])
          max_revisions = int(os.environ["MAX_REVISIONS"])
          no_change_patience = int(os.environ["NO_CHANGE_PATIENCE"])

          pr_comment_path = Path(os.environ["PR_COMMENT_PATH"]).expanduser()
          check_body_path = Path(os.environ["CHECK_BODY_PATH"]).expanduser()

          # ----- load artifact (current results) -----
          artifacts_dir = Path(os.environ["ARTIFACTS_DIR"]).expanduser().resolve()
          if not artifacts_dir.exists():
              pr_comment_path.touch()
              check_body_path.touch()
              sys.exit(0)

          current_txt_path = next(artifacts_dir.rglob("speed_test*.txt"), None)
          if current_txt_path is None:
              pr_comment_path.touch()
              check_body_path.touch()
              sys.exit(0)

          current_bm = artifacts_parse_speed_txt_lines(current_txt_path.read_text(encoding="utf-8").splitlines())

          # ----- W&B baselines -----
          if not "WANDB_API_KEY" in os.environ:
              print("WANDB_API_KEY is not set")
              sys.exit(0)
          ENTITY = os.environ["WANDB_ENTITY"]
          PROJECT = os.environ["WANDB_PROJECT"]

          api = wandb.Api()
          runs_iter = api.runs(f"{ENTITY}/{PROJECT}", order="-created_at")

          by_rev = {}
          rev_order = []
          selected_revs = None
          no_change = 0

          for run in runs_iter:
              if run.state != "finished":
                  continue

              cfg = getattr(run, "config", None)
              if cfg is None:
                  continue

              cfg = json.loads(cfg)
              raw_rev = cfg.get("revision")
              raw_bid = cfg.get("benchmark_id")
              if not raw_rev or not raw_bid:
                  if selected_revs is not None:
                      no_change += 1
                      if no_change >= no_change_patience:
                          break
                  continue

              rev = _norm_rev(raw_rev.get("value"))
              bid = raw_bid.get("value")
              if not rev or not bid:
                  if selected_revs is not None:
                      no_change += 1
                      if no_change >= no_change_patience:
                          break
                  continue

              if (selected_revs is not None) and (rev not in selected_revs):
                  no_change += 1
                  if no_change >= no_change_patience:
                      break
                  continue

              if (rev not in by_rev):
                  by_rev[rev]={}
                  rev_order.append(rev)
                  if len(rev_order) >= max_revisions:
                      selected_revs = set(rev_order)

              nbid = wandb_normalize_benchmark_id(bid)
              if (nbid not in by_rev[rev]):
                  runtime_fps=None
                  compile_time=None
                  cnt=0

                  for row in run.scan_history(keys=["runtime_fps","compile_time"]):
                      runtime_fps = row.get("runtime_fps")
                      compile_time = row.get("compile_time")
                      if (runtime_fps is not None) and (compile_time is not None):
                          break
                      cnt += 1
                      if cnt >= 10:
                          break

                  by_rev[rev][nbid] = {
                      "runtime_fps": runtime_fps,
                      "compile_time": compile_time
                  }
                  if (selected_revs is not None):
                      no_change = 0
              else:
                  if (selected_revs is not None):
                      no_change += 1
                      if no_change >= no_change_patience:
                          break

          def mean_of(metric, bid):
              vals=[]
              for r in by_rev.keys():
                  v = by_rev.get(r, {}).get(bid, {}).get(metric)
                  if isinstance(v,(int,float)) and not (isinstance(v,float) and math.isnan(v)):
                      vals.append(float(v))
              return sum(vals) / max(len(vals), 1)

          # ----- build TWO tables -----
          rows_rt = []
          rows_ct = []
          reg_found = False
          for bid in sorted(current_bm.keys()):
              cur_rt = current_bm[bid].get("runtime_fps")
              cur_ct = current_bm[bid].get("compile_time")
              base_rt = mean_of("runtime_fps", bid)
              base_ct = mean_of("compile_time", bid)

              d_rt = ((cur_rt - base_rt)/base_rt*100.0) if (base_rt and isinstance(cur_rt, (int, float))) else None
              d_ct = ((cur_ct - base_ct)/base_ct*100.0) if (base_ct and isinstance(cur_ct, (int, float))) else None

              is_rt_reg = (d_rt is not None and d_rt < -tol_rt)
              is_ct_reg = (d_ct is not None and d_ct > tol_ct)
              reg_found = reg_found or is_rt_reg or is_ct_reg

              stat_rt = "ðŸ”´" if is_rt_reg else ("â„¹ï¸" if base_rt is None else "âœ…")
              stat_ct = "ðŸ”´" if is_ct_reg else ("â„¹ï¸" if base_ct is None else "âœ…")

              rows_rt.append([
                  stat_rt, f"`{bid}`",
                  fmt_num(cur_rt), fmt_num(base_rt),
                  fmt_pct(d_rt, highlight=is_rt_reg)
              ])
              rows_ct.append([
                  stat_ct, f"`{bid}`",
                  fmt_num(cur_ct), fmt_num(base_ct),
                  fmt_pct(d_ct, highlight=is_ct_reg)
              ])

          header_rt = [
            "| status | benchmark_id | current FPS | baseline FPS | Î” FPS |",
            "|:------:|:-------------|-----------:|-------------:|------:|",
          ]
          header_ct = [
            "| status | benchmark_id | current compile | baseline compile | Î” compile |",
            "|:------:|:-------------|----------------:|-----------------:|---------:|",
          ]
          table_rt = header_rt + ["| " + " | ".join(r) + " |" for r in rows_rt]
          table_ct = header_ct + ["| " + " | ".join(r) + " |" for r in rows_ct]

          # ----- baseline commit list -----
          blist = [f"- Commit {i}: {sha}" for i, sha in enumerate(rev_order, 1)]
          baseline_block = ["**Baselines considered:** " + f"**{len(rev_order)}** commits"] + blist

          # ----- CHECK body (always) -----
          check_parts = []
          check_parts += baseline_block
          check_parts += ["", f"Thresholds: runtime â‰¤ âˆ’{tol_rt:.0f}%, compile â‰¥ +{tol_ct:.0f}%", ""]
          check_parts += ["### Runtime FPS", *table_rt, ""]
          check_parts += ["### Compile Time", *table_ct, ""]
          check_body = "\n".join(check_parts)

          # ----- COMMENT body (only if regressions) -----
          if reg_found:
              comment_parts = [
                  ":warning: **Benchmark Regression Detected**",
                  *baseline_block,
                  "\n",
                  f"Thresholds: runtime â‰¤ âˆ’{tol_rt:.0f}%, compile â‰¥ +{tol_ct:.0f}%",
                  "\n",
                  "### Runtime FPS",
                  *table_rt,
                  "\n",
                  "### Compile Time",
                  *table_ct,
              ]
              comment_body = "\n".join(comment_parts)
          else:
              comment_body = ""

          # write files
          check_body_path.write_text(check_body + "\n", encoding="utf-8")
          pr_comment_path.write_text(comment_body + "\n", encoding="utf-8")

          # exit with 1 if regressions are found, 0 otherwise
          sys.exit(1 if reg_found else 0)
          PY
          rc=$?
          echo "HAS_REGRESSIONS=${rc}" >> "$GITHUB_ENV"
          HAS_REGRESSIONS="${rc}"
          } || true

          # expose outputs to later steps
          if [ -f "check_output.md" ]; then
            {
              echo 'CHECK_OUTPUT<<__EOF__'
              cat check_output.md
              echo '__EOF__'
            } >> "$GITHUB_ENV"
          else
            echo "CHECK_OUTPUT=" >> "$GITHUB_ENV"
          fi

          # only set SCRIPT_OUTPUT when we actually want to comment
          if [ "${HAS_REGRESSIONS}" = "1" ] && [ -s "pr_comment.md" ]; then
            {
              echo 'SCRIPT_OUTPUT<<__EOF__'
              cat pr_comment.md
              echo '__EOF__'
            } >> "$GITHUB_ENV"
          else
            echo "SCRIPT_OUTPUT=" >> "$GITHUB_ENV"
          fi

      - name: Add PR comment
        if: ${{ steps.pr.outputs.pr_number != '' && env.SCRIPT_OUTPUT != '' }}
        uses: actions/github-script@v7
        env:
          COMMENT_BODY: ${{ env.SCRIPT_OUTPUT }}
        with:
          script: |
            const prNumber = Number('${{ steps.pr.outputs.pr_number }}');
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
              body: process.env.COMMENT_BODY
            });

      - name: Publish PR check
        if: always()
        uses: actions/github-script@v7
        env:
          CHECK_NAME: Benchmark Comparison
          CHECK_BODY: ${{ env.CHECK_OUTPUT }}
          HAS_REGRESSIONS: ${{ env.HAS_REGRESSIONS }}
        with:
          script: |
            const sha = context.payload.workflow_run.head_sha;
            const hasRegs = (process.env.HAS_REGRESSIONS || '0').trim() === '1';
            const conclusion = 'success';
            const summary = hasRegs
              ? 'ðŸ”´ Regressions detected. See tables below.'
              : 'âœ… No regressions detected. See tables below.';
            await github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: process.env.CHECK_NAME,
              head_sha: sha,
              status: 'completed',
              conclusion,
              output: {
                title: process.env.CHECK_NAME,
                summary,
                text: process.env.CHECK_BODY || undefined
              }
            });
